import os
import time
import tempfile
from io import BytesIO
from typing import Dict

import pandas as pd
from prefect import flow, task
from prefect.cache_policies import NO_CACHE
from pyspark.sql import DataFrame, SparkSession
from pyspark.sql import functions as F
from pyspark.sql.window import Window

from config import BUCKET_BRONZE, BUCKET_SILVER, get_minio_client

_SPARK_SESSION: SparkSession | None = None


def get_spark_session(app_name: str = "silver-transformation") -> SparkSession:
    global _SPARK_SESSION
    if _SPARK_SESSION is None:
        master_url = os.getenv("SPARK_MASTER_URL", "local[*]")
        _SPARK_SESSION = (
            SparkSession.builder.appName(app_name)
            .master(master_url)
            .config("spark.sql.session.timeZone", "UTC")
            .getOrCreate()
        )
    return _SPARK_SESSION


def _download_object_to_tempfile(bucket: str, object_name: str) -> str:
    client = get_minio_client()
    response = client.get_object(bucket, object_name)
    data = response.read()
    response.close()
    response.release_conn()

    temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=".csv")
    temp_file.write(data)
    temp_file.flush()
    temp_file.close()
    return temp_file.name


def _upload_dataframe_to_minio(df: DataFrame, bucket: str, object_name: str) -> str:
    client = get_minio_client()

    if not client.bucket_exists(bucket):
        client.make_bucket(bucket)

    with tempfile.TemporaryDirectory() as temp_dir:
        df.coalesce(1).write.mode("overwrite").option("header", True).csv(temp_dir)
        part_files = [
            f for f in os.listdir(temp_dir) if f.startswith("part-") and f.endswith(".csv")
        ]
        if not part_files:
            raise FileNotFoundError("No CSV part file generated by Spark")
        part_file = os.path.join(temp_dir, part_files[0])
        client.fput_object(bucket, object_name, part_file)

    print(f"Saved {bucket}/{object_name}")
    return object_name


@task(name="load_from_bronze", retries=2, cache_policy=NO_CACHE)
def load_data_from_bronze(object_name: str) -> DataFrame:
    spark = get_spark_session("silver-load")
    temp_path = _download_object_to_tempfile(BUCKET_BRONZE, object_name)
    df = (
        spark.read.option("header", True)
        .option("inferSchema", True)
        .csv(temp_path)
    )
    print(f"Loaded {df.count()} rows from {object_name}")
    return df


@task(name="clean_clients_data", cache_policy=NO_CACHE)
def clean_clients_data(df: DataFrame) -> DataFrame:
    print(f"Initial rows: {df.count()}")

    df_clean = (
        df.dropna(subset=["id_client", "email"])
        .withColumn("name", F.coalesce(F.col("name"), F.lit("Unknown")))
        .withColumn("country", F.coalesce(F.col("country"), F.lit("Unknown")))
        .withColumn("date_inscription", F.to_date(F.col("date_inscription")))
        .dropna(subset=["date_inscription"])
        .withColumn("has_future_date", F.col("date_inscription") > F.current_date())
        .withColumn("id_client", F.col("id_client").cast("int"))
        .withColumn("email", F.lower(F.trim(F.col("email"))))
        .withColumn("country", F.trim(F.col("country")))
        .withColumn("name", F.trim(F.col("name")))
    )

    email_pattern = r"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$"
    df_clean = df_clean.withColumn("email_valid", F.col("email").rlike(email_pattern))

    df_clean = df_clean.dropDuplicates()

    window = Window.partitionBy("email").orderBy(F.col("date_inscription").desc())
    df_clean = df_clean.withColumn("rn", F.row_number().over(window))
    df_clean = df_clean.filter(F.col("rn") == 1).drop("rn")

    df_clean = df_clean.withColumn("cleaned_at", F.current_timestamp())
    df_clean = df_clean.withColumn(
        "data_quality_score",
        (
            F.col("email_valid").cast("int") * F.lit(0.4)
            + (~F.col("has_future_date")).cast("int") * F.lit(0.3)
            + (F.col("name") != F.lit("Unknown")).cast("int") * F.lit(0.2)
            + (F.col("country") != F.lit("Unknown")).cast("int") * F.lit(0.1)
        ),
    )

    print(f"Final cleaned rows: {df_clean.count()}")
    return df_clean


@task(name="clean_achats_data", cache_policy=NO_CACHE)
def clean_achats_data(df: DataFrame) -> DataFrame:
    print(f"Initial rows: {df.count()}")

    df_clean = (
        df.dropna(subset=["id_achat", "id_client", "montant"])
        .withColumn("produit", F.coalesce(F.col("produit"), F.lit("Unknown")))
        .withColumn("date_achat", F.to_date(F.col("date_achat")))
        .dropna(subset=["date_achat"])
        .withColumn("has_future_date", F.col("date_achat") > F.current_date())
        .withColumn("id_achat", F.col("id_achat").cast("int"))
        .withColumn("id_client", F.col("id_client").cast("int"))
        .withColumn("montant", F.col("montant").cast("double"))
        .filter(F.col("montant") > 0)
        .withColumn("produit", F.initcap(F.trim(F.col("produit"))))
    )

    quantiles = df_clean.approxQuantile("montant", [0.25, 0.75], 0.01)
    if len(quantiles) == 2:
        q1, q3 = quantiles
        iqr = q3 - q1
        lower_bound = q1 - 3 * iqr
        upper_bound = q3 + 3 * iqr
        df_clean = df_clean.withColumn(
            "is_outlier",
            (F.col("montant") < F.lit(lower_bound))
            | (F.col("montant") > F.lit(upper_bound)),
        )
    else:
        df_clean = df_clean.withColumn("is_outlier", F.lit(False))

    df_clean = df_clean.dropDuplicates()
    df_clean = df_clean.dropDuplicates(
        subset=["id_client", "date_achat", "montant", "produit"]
    )

    df_clean = df_clean.withColumn("cleaned_at", F.current_timestamp())
    df_clean = df_clean.withColumn(
        "data_quality_score",
        (
            (~F.col("is_outlier")).cast("int") * F.lit(0.5)
            + (~F.col("has_future_date")).cast("int") * F.lit(0.3)
            + (F.col("produit") != F.lit("Unknown")).cast("int") * F.lit(0.2)
        ),
    )

    print(f"Final cleaned rows: {df_clean.count()}")
    return df_clean


@task(name="save_to_silver", retries=2, cache_policy=NO_CACHE)
def save_to_silver(df: DataFrame, object_name: str) -> str:
    return _upload_dataframe_to_minio(df, BUCKET_SILVER, object_name)


def _pandas_silver_benchmark() -> Dict[str, float]:
    start_time = time.perf_counter()
    client = get_minio_client()

    def load_pandas(object_name: str) -> pd.DataFrame:
        response = client.get_object(BUCKET_BRONZE, object_name)
        data = response.read()
        response.close()
        response.release_conn()
        return pd.read_csv(BytesIO(data))

    clients_df = load_pandas("clients.csv")
    achats_df = load_pandas("achats.csv")

    clients_df = clients_df.dropna(subset=["id_client", "email"])
    clients_df["name"] = clients_df["name"].fillna("Unknown")
    clients_df["country"] = clients_df["country"].fillna("Unknown")
    clients_df["date_inscription"] = pd.to_datetime(
        clients_df["date_inscription"], errors="coerce"
    )
    clients_df = clients_df.dropna(subset=["date_inscription"])
    clients_df["has_future_date"] = clients_df["date_inscription"] > pd.Timestamp.now()
    clients_df["id_client"] = clients_df["id_client"].astype(int)
    clients_df["email"] = clients_df["email"].str.lower().str.strip()
    clients_df["country"] = clients_df["country"].str.strip()
    clients_df["name"] = clients_df["name"].str.strip()
    email_pattern = r"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$"
    clients_df["email_valid"] = clients_df["email"].str.match(email_pattern, na=False)
    clients_df = clients_df.drop_duplicates()
    clients_df = clients_df.sort_values("date_inscription", ascending=False)
    clients_df = clients_df.drop_duplicates(subset=["email"], keep="first")

    achats_df = achats_df.dropna(subset=["id_achat", "id_client", "montant"])
    achats_df["produit"] = achats_df["produit"].fillna("Unknown")
    achats_df["date_achat"] = pd.to_datetime(achats_df["date_achat"], errors="coerce")
    achats_df = achats_df.dropna(subset=["date_achat"])
    achats_df["has_future_date"] = achats_df["date_achat"] > pd.Timestamp.now()
    achats_df["id_achat"] = achats_df["id_achat"].astype(int)
    achats_df["id_client"] = achats_df["id_client"].astype(int)
    achats_df["montant"] = achats_df["montant"].astype(float)
    achats_df = achats_df[achats_df["montant"] > 0]
    achats_df["produit"] = achats_df["produit"].str.strip().str.title()
    achats_df = achats_df.drop_duplicates()
    achats_df = achats_df.drop_duplicates(
        subset=["id_client", "date_achat", "montant", "produit"], keep="first"
    )

    duration = time.perf_counter() - start_time
    return {"pandas_duration_seconds": duration}


@flow(name="Silver Transformation Flow")
def silver_transformation_flow(run_benchmark: bool = True) -> dict:
    spark_start = time.perf_counter()

    clients_df = load_data_from_bronze("clients.csv")
    achats_df = load_data_from_bronze("achats.csv")

    clients_clean = clean_clients_data(clients_df)
    achats_clean = clean_achats_data(achats_df)

    save_to_silver(clients_clean, "clients_clean.csv")
    save_to_silver(achats_clean, "achats_clean.csv")

    spark_duration = time.perf_counter() - spark_start

    stats = {
        "clients": {
            "bronze_rows": clients_df.count(),
            "silver_rows": clients_clean.count(),
            "rows_removed": clients_df.count() - clients_clean.count(),
            "removal_rate": (
                (clients_df.count() - clients_clean.count()) / clients_df.count() * 100
            ),
            "avg_quality_score": float(
                clients_clean.agg(F.avg("data_quality_score")).collect()[0][0]
            ),
        },
        "achats": {
            "bronze_rows": achats_df.count(),
            "silver_rows": achats_clean.count(),
            "rows_removed": achats_df.count() - achats_clean.count(),
            "removal_rate": (
                (achats_df.count() - achats_clean.count()) / achats_df.count() * 100
            ),
            "avg_quality_score": float(
                achats_clean.agg(F.avg("data_quality_score")).collect()[0][0]
            ),
        },
        "spark_duration_seconds": spark_duration,
    }

    if run_benchmark:
        stats.update(_pandas_silver_benchmark())

    return stats


if __name__ == "__main__":
    result = silver_transformation_flow(run_benchmark=True)
    print("\n=== Silver Transformation Complete ===")
    print(
        f"Spark duration: {result['spark_duration_seconds']:.2f}s | "
        f"Pandas duration: {result.get('pandas_duration_seconds', 0):.2f}s"
    )


def _upload_dataframe_to_minio(df: DataFrame, bucket: str, object_name: str) -> str:
    client = get_minio_client()

    if not client.bucket_exists(bucket):
        client.make_bucket(bucket)

    with tempfile.TemporaryDirectory() as temp_dir:
        df.coalesce(1).write.mode("overwrite").option("header", True).csv(temp_dir)
        part_files = [
            f for f in os.listdir(temp_dir) if f.startswith("part-") and f.endswith(".csv")
        ]
        if not part_files:
            raise FileNotFoundError("No CSV part file generated by Spark")
        part_file = os.path.join(temp_dir, part_files[0])
        client.fput_object(bucket, object_name, part_file)

    print(f"Saved {bucket}/{object_name}")
    return object_name


@task(name="load_from_bronze", retries=2)
def load_data_from_bronze(object_name: str) -> DataFrame:
    """
    Load CSV data from bronze bucket using Spark.
    """
    spark = get_spark_session("silver-load")
    temp_path = _download_object_to_tempfile(BUCKET_BRONZE, object_name)
    df = (
        spark.read.option("header", True)
        .option("inferSchema", True)
        .csv(temp_path)
    )
    print(f"Loaded {df.count()} rows from {object_name}")
    return df


@task(name="clean_clients_data")
def clean_clients_data(df: DataFrame) -> DataFrame:
    """
    Clean and standardize clients data using Spark.
    """
    print(f"Initial rows: {df.count()}")

    df_clean = (
        df.dropna(subset=["id_client", "email"])
        .withColumn("name", F.coalesce(F.col("name"), F.lit("Unknown")))
        .withColumn("country", F.coalesce(F.col("country"), F.lit("Unknown")))
        .withColumn("date_inscription", F.to_date(F.col("date_inscription")))
        .dropna(subset=["date_inscription"])
        .withColumn("has_future_date", F.col("date_inscription") > F.current_date())
        .withColumn("id_client", F.col("id_client").cast("int"))
        .withColumn("email", F.lower(F.trim(F.col("email"))))
        .withColumn("country", F.trim(F.col("country")))
        .withColumn("name", F.trim(F.col("name")))
    )

    email_pattern = r"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$"
    df_clean = df_clean.withColumn("email_valid", F.col("email").rlike(email_pattern))

    df_clean = df_clean.dropDuplicates()

    window = Window.partitionBy("email").orderBy(F.col("date_inscription").desc())
    df_clean = df_clean.withColumn("rn", F.row_number().over(window))
    df_clean = df_clean.filter(F.col("rn") == 1).drop("rn")

    df_clean = df_clean.withColumn("cleaned_at", F.current_timestamp())

    df_clean = df_clean.withColumn(
        "data_quality_score",
        (
            F.col("email_valid").cast("int") * F.lit(0.4)
            + (~F.col("has_future_date")).cast("int") * F.lit(0.3)
            + (F.col("name") != F.lit("Unknown")).cast("int") * F.lit(0.2)
            + (F.col("country") != F.lit("Unknown")).cast("int") * F.lit(0.1)
        ),
    )

    print(f"Final cleaned rows: {df_clean.count()}")
    return df_clean


@task(name="clean_achats_data")
def clean_achats_data(df: DataFrame) -> DataFrame:
    """
    Clean and standardize purchases (achats) data using Spark.
    """
    print(f"Initial rows: {df.count()}")

    df_clean = (
        df.dropna(subset=["id_achat", "id_client", "montant"])
        .withColumn("produit", F.coalesce(F.col("produit"), F.lit("Unknown")))
        .withColumn("date_achat", F.to_date(F.col("date_achat")))
        .dropna(subset=["date_achat"])
        .withColumn("has_future_date", F.col("date_achat") > F.current_date())
        .withColumn("id_achat", F.col("id_achat").cast("int"))
        .withColumn("id_client", F.col("id_client").cast("int"))
        .withColumn("montant", F.col("montant").cast("double"))
        .filter(F.col("montant") > 0)
        .withColumn("produit", F.initcap(F.trim(F.col("produit"))))
    )

    quantiles = df_clean.approxQuantile("montant", [0.25, 0.75], 0.01)
    if len(quantiles) == 2:
        q1, q3 = quantiles
        iqr = q3 - q1
        lower_bound = q1 - 3 * iqr
        upper_bound = q3 + 3 * iqr
        df_clean = df_clean.withColumn(
            "is_outlier",
            (F.col("montant") < F.lit(lower_bound)) | (F.col("montant") > F.lit(upper_bound)),
        )
    else:
        df_clean = df_clean.withColumn("is_outlier", F.lit(False))

    df_clean = df_clean.dropDuplicates()
    df_clean = df_clean.dropDuplicates(
        subset=["id_client", "date_achat", "montant", "produit"]
    )

    df_clean = df_clean.withColumn("cleaned_at", F.current_timestamp())
    df_clean = df_clean.withColumn(
        "data_quality_score",
        (
            (~F.col("is_outlier")).cast("int") * F.lit(0.5)
            + (~F.col("has_future_date")).cast("int") * F.lit(0.3)
            + (F.col("produit") != F.lit("Unknown")).cast("int") * F.lit(0.2)
        ),
    )

    print(f"Final cleaned rows: {df_clean.count()}")
    return df_clean


@task(name="save_to_silver", retries=2)
def save_to_silver(df: DataFrame, object_name: str) -> str:
    """
    Save cleaned Spark DataFrame to silver bucket.
    """
    return _upload_dataframe_to_minio(df, BUCKET_SILVER, object_name)


def _pandas_silver_benchmark() -> Dict[str, float]:
    """
    Run the legacy pandas pipeline to compare processing time.
    """
    start_time = time.perf_counter()
    client = get_minio_client()

    def load_pandas(object_name: str) -> pd.DataFrame:
        response = client.get_object(BUCKET_BRONZE, object_name)
        data = response.read()
        response.close()
        response.release_conn()
        return pd.read_csv(BytesIO(data))

    clients_df = load_pandas("clients.csv")
    achats_df = load_pandas("achats.csv")

    # Minimal transformation using the same logic as before
    clients_df = clients_df.dropna(subset=["id_client", "email"])
    clients_df["name"] = clients_df["name"].fillna("Unknown")
    clients_df["country"] = clients_df["country"].fillna("Unknown")
    clients_df["date_inscription"] = pd.to_datetime(
        clients_df["date_inscription"], errors="coerce"
    )
    clients_df = clients_df.dropna(subset=["date_inscription"])
    clients_df["has_future_date"] = clients_df["date_inscription"] > pd.Timestamp.now()
    clients_df["id_client"] = clients_df["id_client"].astype(int)
    clients_df["email"] = clients_df["email"].str.lower().str.strip()
    clients_df["country"] = clients_df["country"].str.strip()
    clients_df["name"] = clients_df["name"].str.strip()
    email_pattern = r"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$"
    clients_df["email_valid"] = clients_df["email"].str.match(email_pattern, na=False)
    clients_df = clients_df.drop_duplicates()
    clients_df = clients_df.sort_values("date_inscription", ascending=False)
    clients_df = clients_df.drop_duplicates(subset=["email"], keep="first")

    achats_df = achats_df.dropna(subset=["id_achat", "id_client", "montant"])
    achats_df["produit"] = achats_df["produit"].fillna("Unknown")
    achats_df["date_achat"] = pd.to_datetime(achats_df["date_achat"], errors="coerce")
    achats_df = achats_df.dropna(subset=["date_achat"])
    achats_df["has_future_date"] = achats_df["date_achat"] > pd.Timestamp.now()
    achats_df["id_achat"] = achats_df["id_achat"].astype(int)
    achats_df["id_client"] = achats_df["id_client"].astype(int)
    achats_df["montant"] = achats_df["montant"].astype(float)
    achats_df = achats_df[achats_df["montant"] > 0]
    achats_df["produit"] = achats_df["produit"].str.strip().str.title()
    achats_df = achats_df.drop_duplicates()
    achats_df = achats_df.drop_duplicates(
        subset=["id_client", "date_achat", "montant", "produit"], keep="first"
    )

    duration = time.perf_counter() - start_time
    return {"pandas_duration_seconds": duration}


@flow(name="Silver Transformation Flow")
def silver_transformation_flow(run_benchmark: bool = True) -> dict:
    """
    Main flow: Load from bronze, clean data with Spark, and save to silver layer.
    """
    spark_start = time.perf_counter()

    clients_df = load_data_from_bronze("clients.csv")
    achats_df = load_data_from_bronze("achats.csv")

    clients_clean = clean_clients_data(clients_df)
    achats_clean = clean_achats_data(achats_df)

    save_to_silver(clients_clean, "clients_clean.csv")
    save_to_silver(achats_clean, "achats_clean.csv")

    spark_duration = time.perf_counter() - spark_start

    stats = {
        "clients": {
            "bronze_rows": clients_df.count(),
            "silver_rows": clients_clean.count(),
            "rows_removed": clients_df.count() - clients_clean.count(),
            "removal_rate": (
                (clients_df.count() - clients_clean.count()) / clients_df.count() * 100
            ),
            "avg_quality_score": float(
                clients_clean.agg(F.avg("data_quality_score")).collect()[0][0]
            ),
        },
        "achats": {
            "bronze_rows": achats_df.count(),
            "silver_rows": achats_clean.count(),
            "rows_removed": achats_df.count() - achats_clean.count(),
            "removal_rate": (
                (achats_df.count() - achats_clean.count()) / achats_df.count() * 100
            ),
            "avg_quality_score": float(
                achats_clean.agg(F.avg("data_quality_score")).collect()[0][0]
            ),
        },
        "spark_duration_seconds": spark_duration,
    }

    if run_benchmark:
        stats.update(_pandas_silver_benchmark())

    return stats


if __name__ == "__main__":
    result = silver_transformation_flow(run_benchmark=True)
    print("\n=== Silver Transformation Complete ===")
    print(
        f"Spark duration: {result['spark_duration_seconds']:.2f}s | "
        f"Pandas duration: {result.get('pandas_duration_seconds', 0):.2f}s"
    )
