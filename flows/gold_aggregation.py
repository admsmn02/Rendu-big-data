import os
import time
import tempfile
from io import BytesIO
from typing import Dict

import pandas as pd
from prefect import flow, task
from prefect.cache_policies import NO_CACHE
from pyspark.sql import DataFrame, SparkSession
from pyspark.sql import functions as F
from pyspark.sql.window import Window

from config import BUCKET_SILVER, BUCKET_GOLD, get_minio_client

_SPARK_SESSION: SparkSession | None = None


def get_spark_session(app_name: str = "gold-aggregation") -> SparkSession:
    global _SPARK_SESSION
    if _SPARK_SESSION is None:
        master_url = os.getenv("SPARK_MASTER_URL", "local[*]")
        _SPARK_SESSION = (
            SparkSession.builder.appName(app_name)
            .master(master_url)
            .config("spark.sql.session.timeZone", "UTC")
            .getOrCreate()
        )
    return _SPARK_SESSION


def _download_object_to_tempfile(bucket: str, object_name: str) -> str:
    client = get_minio_client()
    response = client.get_object(bucket, object_name)
    data = response.read()
    response.close()
    response.release_conn()

    temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=".csv")
    temp_file.write(data)
    temp_file.flush()
    temp_file.close()
    return temp_file.name


def _upload_dataframe_to_minio(df: DataFrame, bucket: str, object_name: str) -> str:
    client = get_minio_client()

    if not client.bucket_exists(bucket):
        client.make_bucket(bucket)

    with tempfile.TemporaryDirectory() as temp_dir:
        df.coalesce(1).write.mode("overwrite").option("header", True).csv(temp_dir)
        part_files = [
            f for f in os.listdir(temp_dir) if f.startswith("part-") and f.endswith(".csv")
        ]
        if not part_files:
            raise FileNotFoundError("No CSV part file generated by Spark")
        part_file = os.path.join(temp_dir, part_files[0])
        client.fput_object(bucket, object_name, part_file)

    print(f"Saved {bucket}/{object_name}")
    return object_name


@task(name="load_from_silver", retries=2, cache_policy=NO_CACHE)
def load_data_from_silver(object_name: str) -> DataFrame:
    """
    Load cleaned CSV data from silver bucket using Spark.
    """
    spark = get_spark_session("gold-load")
    temp_path = _download_object_to_tempfile(BUCKET_SILVER, object_name)
    df = (
        spark.read.option("header", True)
        .option("inferSchema", True)
        .csv(temp_path)
    )

    if "date_inscription" in df.columns:
        df = df.withColumn("date_inscription", F.to_date(F.col("date_inscription")))
    if "date_achat" in df.columns:
        df = df.withColumn("date_achat", F.to_date(F.col("date_achat")))
    if "cleaned_at" in df.columns:
        df = df.withColumn("cleaned_at", F.to_timestamp(F.col("cleaned_at")))

    print(f"Loaded {df.count()} rows from {object_name}")
    return df


@task(name="create_fact_achats", cache_policy=NO_CACHE)
def create_fact_achats(achats_df: DataFrame, clients_df: DataFrame) -> DataFrame:
    """
    Create fact table for purchases with enriched dimensions.
    """
    print("Creating fact_achats table...")

    fact = achats_df.join(
        clients_df.select("id_client", "country", "date_inscription"),
        on="id_client",
        how="left",
    )

    fact = (
        fact.withColumn("year", F.year(F.col("date_achat")))
        .withColumn("month", F.month(F.col("date_achat")))
        .withColumn("quarter", F.quarter(F.col("date_achat")))
        .withColumn("week", F.weekofyear(F.col("date_achat")))
        .withColumn("day_of_week", F.dayofweek(F.col("date_achat")))
        .withColumn("day_of_week_name", F.date_format(F.col("date_achat"), "EEEE"))
        .withColumn("year_month", F.date_format(F.col("date_achat"), "yyyy-MM"))
        .withColumn(
            "year_week",
            F.concat_ws(
                "-W",
                F.year(F.col("date_achat")),
                F.lpad(F.weekofyear(F.col("date_achat")).cast("string"), 2, "0"),
            ),
        )
        .withColumn("client_age_days", F.datediff(F.col("date_achat"), F.col("date_inscription")))
        .withColumn("client_age_months", F.col("client_age_days") / F.lit(30.44))
    )

    threshold = fact.approxQuantile("montant", [0.75], 0.01)
    high_value_threshold = threshold[0] if threshold else 0
    fact = fact.withColumn("is_high_value", F.col("montant") > F.lit(high_value_threshold))

    print(f"Created fact table with {fact.count()} rows")
    return fact


@task(name="calculate_kpis_by_period", cache_policy=NO_CACHE)
def calculate_kpis_by_period(fact_df: DataFrame) -> DataFrame:
    print("Calculating KPIs by period...")

    monthly_kpis = (
        fact_df.groupBy("year", "month", "year_month")
        .agg(
            F.count("id_achat").alias("volume_achats"),
            F.sum("montant").alias("ca_total"),
            F.mean("montant").alias("panier_moyen"),
            F.stddev("montant").alias("panier_std"),
            F.min("montant").alias("panier_min"),
            F.max("montant").alias("panier_max"),
            F.countDistinct("id_client").alias("nb_clients_uniques"),
            F.countDistinct("produit").alias("nb_produits_uniques"),
        )
        .orderBy("year", "month")
    )

    window = Window.orderBy("year", "month")
    monthly_kpis = (
        monthly_kpis.withColumn(
            "ca_growth_rate",
            (F.col("ca_total") / F.lag("ca_total").over(window) - 1) * 100,
        )
        .withColumn(
            "volume_growth_rate",
            (F.col("volume_achats") / F.lag("volume_achats").over(window) - 1)
            * 100,
        )
        .withColumn("period_type", F.lit("monthly"))
    )

    print(f"Generated {monthly_kpis.count()} monthly KPI records")
    return monthly_kpis


@task(name="calculate_kpis_by_country", cache_policy=NO_CACHE)
def calculate_kpis_by_country(fact_df: DataFrame) -> DataFrame:
    print("Calculating KPIs by country...")

    country_kpis = fact_df.groupBy("country").agg(
        F.count("id_achat").alias("volume_achats"),
        F.sum("montant").alias("ca_total"),
        F.mean("montant").alias("panier_moyen"),
        F.stddev("montant").alias("panier_std"),
        F.countDistinct("id_client").alias("nb_clients_uniques"),
        F.countDistinct("produit").alias("nb_produits_uniques"),
    )

    total_ca = country_kpis.agg(F.sum("ca_total")).collect()[0][0]
    country_kpis = (
        country_kpis.withColumn(
            "market_share", (F.col("ca_total") / F.lit(total_ca)) * 100
        )
        .withColumn(
            "ca_per_client",
            F.col("ca_total") / F.col("nb_clients_uniques"),
        )
        .orderBy(F.col("ca_total").desc())
    )

    print(f"Generated KPIs for {country_kpis.count()} countries")
    return country_kpis


@task(name="calculate_kpis_by_product", cache_policy=NO_CACHE)
def calculate_kpis_by_product(fact_df: DataFrame) -> DataFrame:
    print("Calculating KPIs by product...")

    product_kpis = fact_df.groupBy("produit").agg(
        F.count("id_achat").alias("volume_achats"),
        F.sum("montant").alias("ca_total"),
        F.mean("montant").alias("panier_moyen"),
        F.stddev("montant").alias("panier_std"),
        F.min("montant").alias("panier_min"),
        F.max("montant").alias("panier_max"),
        F.countDistinct("id_client").alias("nb_clients_uniques"),
    )

    total_ca = product_kpis.agg(F.sum("ca_total")).collect()[0][0]
    total_volume = product_kpis.agg(F.sum("volume_achats")).collect()[0][0]

    product_kpis = (
        product_kpis.withColumn(
            "market_share_ca", (F.col("ca_total") / F.lit(total_ca)) * 100
        )
        .withColumn(
            "market_share_volume", (F.col("volume_achats") / F.lit(total_volume)) * 100
        )
        .orderBy(F.col("ca_total").desc())
    )

    print(f"Generated KPIs for {product_kpis.count()} products")
    return product_kpis


@task(name="calculate_cohort_analysis", cache_policy=NO_CACHE)
def calculate_cohort_analysis(fact_df: DataFrame) -> DataFrame:
    print("Performing cohort analysis...")

    cohort_data = (
        fact_df.withColumn("cohort", F.date_format(F.col("date_inscription"), "yyyy-MM"))
        .groupBy("cohort", "year_month")
        .agg(
            F.count("id_achat").alias("nb_achats"),
            F.sum("montant").alias("ca_total"),
            F.countDistinct("id_client").alias("nb_clients"),
        )
        .orderBy("cohort", "year_month")
    )

    print(f"Generated cohort analysis with {cohort_data.count()} records")
    return cohort_data


@task(name="calculate_client_segmentation", cache_policy=NO_CACHE)
def calculate_client_segmentation(fact_df: DataFrame) -> DataFrame:
    print("Calculating client segmentation...")

    reference_date = fact_df.agg(F.max("date_achat")).collect()[0][0]

    client_rfm = fact_df.groupBy("id_client").agg(
        F.max("date_achat").alias("last_purchase"),
        F.count("id_achat").alias("frequency"),
        F.sum("montant").alias("monetary_value"),
        F.first("country").alias("country"),
    )

    client_rfm = client_rfm.withColumn(
        "recency_days", F.datediff(F.lit(reference_date), F.col("last_purchase"))
    )

    recency_window = Window.orderBy(F.col("recency_days").asc())
    frequency_window = Window.orderBy(F.col("frequency").asc())
    monetary_window = Window.orderBy(F.col("monetary_value").asc())

    client_rfm = (
        client_rfm.withColumn("recency_bucket", F.ntile(4).over(recency_window))
        .withColumn("frequency_score", F.ntile(4).over(frequency_window))
        .withColumn("monetary_score", F.ntile(4).over(monetary_window))
        .withColumn("recency_score", F.lit(5) - F.col("recency_bucket"))
        .drop("recency_bucket")
    )

    client_rfm = client_rfm.withColumn(
        "rfm_score",
        F.col("recency_score") + F.col("frequency_score") + F.col("monetary_score"),
    )

    client_rfm = client_rfm.withColumn(
        "segment",
        F.when(F.col("rfm_score") >= 10, "Champions")
        .when(F.col("rfm_score") >= 8, "Loyal Customers")
        .when(F.col("rfm_score") >= 6, "Potential Loyalists")
        .when((F.col("rfm_score") >= 5) & (F.col("recency_score") >= 3), "Recent Customers")
        .when(F.col("rfm_score") >= 5, "At Risk")
        .otherwise("Lost"),
    )

    print(f"Segmented {client_rfm.count()} clients")
    return client_rfm


@task(name="calculate_weekly_trends", cache_policy=NO_CACHE)
def calculate_weekly_trends(fact_df: DataFrame) -> DataFrame:
    print("Calculating weekly trends...")

    weekly_kpis = (
        fact_df.groupBy("year", "week", "year_week")
        .agg(
            F.count("id_achat").alias("volume_achats"),
            F.sum("montant").alias("ca_total"),
            F.mean("montant").alias("panier_moyen"),
            F.countDistinct("id_client").alias("nb_clients_uniques"),
        )
        .orderBy("year", "week")
    )

    window = Window.orderBy("year", "week")
    weekly_kpis = (
        weekly_kpis.withColumn(
            "ca_growth_rate",
            (F.col("ca_total") / F.lag("ca_total").over(window) - 1) * 100,
        )
        .withColumn("period_type", F.lit("weekly"))
    )

    print(f"Generated {weekly_kpis.count()} weekly trend records")
    return weekly_kpis


@task(name="save_to_gold", retries=2, cache_policy=NO_CACHE)
def save_to_gold(df: DataFrame, object_name: str) -> str:
    """
    Save aggregated Spark DataFrame to gold bucket.
    """
    return _upload_dataframe_to_minio(df, BUCKET_GOLD, object_name)


def _pandas_gold_benchmark() -> Dict[str, float]:
    """
    Run the legacy pandas pipeline to compare processing time.
    """
    start_time = time.perf_counter()
    client = get_minio_client()

    def load_pandas(object_name: str) -> pd.DataFrame:
        response = client.get_object(BUCKET_SILVER, object_name)
        data = response.read()
        response.close()
        response.release_conn()
        df = pd.read_csv(BytesIO(data))
        if "date_inscription" in df.columns:
            df["date_inscription"] = pd.to_datetime(df["date_inscription"])
        if "date_achat" in df.columns:
            df["date_achat"] = pd.to_datetime(df["date_achat"])
        return df

    clients_df = load_pandas("clients_clean.csv")
    achats_df = load_pandas("achats_clean.csv")

    fact = achats_df.merge(
        clients_df[["id_client", "country", "date_inscription"]],
        on="id_client",
        how="left",
    )
    fact["year"] = fact["date_achat"].dt.year
    fact["month"] = fact["date_achat"].dt.month
    fact["quarter"] = fact["date_achat"].dt.quarter
    fact["week"] = fact["date_achat"].dt.isocalendar().week
    fact["year_month"] = fact["date_achat"].dt.to_period("M").astype(str)
    fact["year_week"] = fact["date_achat"].dt.to_period("W").astype(str)

    _ = (
        fact.groupby(["year", "month", "year_month"])["montant"]
        .agg(["sum", "mean"])
        .reset_index()
    )

    duration = time.perf_counter() - start_time
    return {"pandas_duration_seconds": duration}


@flow(name="Gold Aggregation Flow")
def gold_aggregation_flow(run_benchmark: bool = True) -> dict:
    """
    Main flow: Load from silver, create aggregations, and save to gold layer.
    """
    spark_start = time.perf_counter()

    clients_df = load_data_from_silver("clients_clean.csv")
    achats_df = load_data_from_silver("achats_clean.csv")

    fact_achats = create_fact_achats(achats_df, clients_df)
    save_to_gold(fact_achats, "fact_achats.csv")

    monthly_kpis = calculate_kpis_by_period(fact_achats)
    save_to_gold(monthly_kpis, "kpis_monthly.csv")

    weekly_kpis = calculate_weekly_trends(fact_achats)
    save_to_gold(weekly_kpis, "kpis_weekly.csv")

    country_kpis = calculate_kpis_by_country(fact_achats)
    save_to_gold(country_kpis, "kpis_by_country.csv")

    product_kpis = calculate_kpis_by_product(fact_achats)
    save_to_gold(product_kpis, "kpis_by_product.csv")

    cohort_analysis = calculate_cohort_analysis(fact_achats)
    save_to_gold(cohort_analysis, "cohort_analysis.csv")

    client_segments = calculate_client_segmentation(fact_achats)
    save_to_gold(client_segments, "client_segmentation.csv")

    spark_duration = time.perf_counter() - spark_start

    stats = {
        "fact_table_rows": fact_achats.count(),
        "monthly_periods": monthly_kpis.count(),
        "weekly_periods": weekly_kpis.count(),
        "countries": country_kpis.count(),
        "products": product_kpis.count(),
        "cohorts": cohort_analysis.count(),
        "clients_segmented": client_segments.count(),
        "total_revenue": float(fact_achats.agg(F.sum("montant")).collect()[0][0]),
        "total_purchases": fact_achats.count(),
        "unique_clients": fact_achats.select("id_client").distinct().count(),
        "avg_basket": float(fact_achats.agg(F.mean("montant")).collect()[0][0]),
        "date_range": {
            "start": str(fact_achats.agg(F.min("date_achat")).collect()[0][0]),
            "end": str(fact_achats.agg(F.max("date_achat")).collect()[0][0]),
        },
        "spark_duration_seconds": spark_duration,
    }

    if run_benchmark:
        stats.update(_pandas_gold_benchmark())

    return stats


if __name__ == "__main__":
    result = gold_aggregation_flow(run_benchmark=True)
    print("\n=== Gold Aggregation Complete ===")
    print(
        f"Spark duration: {result['spark_duration_seconds']:.2f}s | "
        f"Pandas duration: {result.get('pandas_duration_seconds', 0):.2f}s"
    )
